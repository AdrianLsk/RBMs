{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Units  \n",
    "Stepped sigmoid units (SSU) for binomial units which represent N separate copies of a binary unit sharing the same bias and weights:\n",
    "$$ \\sum\\limits^N_{i=1} σ(x − i + 0.5) \\rightarrow \\log(1 + e^x), \\text{ as } N \\rightarrow \\infty$$\n",
    "Extension to infinite number of copies gives the softplus unit:\n",
    "$$h \\sim \\log(1 + e^x)=softplus(x)$$ for $x = v^T W + b$, where softplus can be approximated by a Noisy ReLU (NReLU) $$max(0, x + N(0, \\sigma(x))$$ or ReLU $$max(0,x)$$  \n",
    "For only N copies, the softplus becomes $$\\log(1 + e^{x}) - \\log(1 + e^{x-N})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8f6b8dd3d0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyjYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f56c98e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.arange(-10, 11)\n",
    "plt.plot(x, np.log(1 + np.exp(x)), 'y', label='softplus')\n",
    "plt.plot(x, np.log(1 + np.exp(x)) - np.log(1 + np.exp(x-len(x))), 'b', label='softplus')\n",
    "plt.plot(x, np.maximum(0, x), 'r', label='ReLU')\n",
    "plt.plot(x, np.maximum(0, x + np.random.randn(21)*np.std(x)), 'g', label='NReLU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:  \n",
    "http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from theano import shared, function, tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "num_vis = 15\n",
    "num_hid = 5\n",
    "num_slab = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike-&-Slab RBM (ssRBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = T.fmatrix('v') # (batch_size, num_vis)\n",
    "h = T.fmatrix('h') # (batch_size, num_hid)\n",
    "s = T.fmatrix('s') # (batch_size, num_hid, num_slab)\n",
    "\n",
    "W = shared(np.random.rand(num_vis, num_hid, num_slab), allow_downcast=True)\n",
    "# diagonal matrices in form of vector, use T.linalg./np.diag for full matrix\n",
    "Lambda = shared(np.random.rand(num_vis), allow_downcast=True)\n",
    "alpha = shared(np.random.rand(num_hid, num_slab), allow_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy function:  \n",
    "$$E(v, s, h) = {1 \\over 2} v^T \\Lambda v - \\sum^N_{i=1} \\left(v^T W_i s_i h_i - {1 \\over 2} s_i^T \\alpha_i s_i + b_i h_i \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visible distribution:  \n",
    "$$p(v, s, h) \\propto \\exp\\left(- E(v, s, h)\\right)$$  \n",
    "$$p(v) \\propto \\sum_{h \\in \\{0,1\\}} \\int_{-\\infty}^\\infty \\exp\\left(- E(v, s, h)\\right) \\mathrm{d}s \\\\\n",
    "= \\int_{-\\infty}^\\infty \\exp\\left( - {1 \\over 2} v^T \\Lambda v \\right) \\prod_{i=1}^N \\sum_{h_i \\in \\{0,1\\}} \\exp\\left(v^T W_i s_i h_i - {1 \\over 2} s_i^T \\alpha_i s_i + b_i h_i \\right) \\mathrm{d}s \\\\  \n",
    "= \\exp\\left( - {1 \\over 2} v^T \\Lambda v \\right) \\prod_{i=1}^N \\sum_{h_i \\in \\{0,1\\}} \\exp\\left( b_i h_i \\right) \\int_{-\\infty}^\\infty \\exp\\left(v^T W_i s_i h_i - {1 \\over 2} s_i^T \\alpha_i s_i \\right) \\mathrm{d}s \\\\ \n",
    "= \\exp\\left( - {1 \\over 2} v^T \\Lambda v \\right) \\prod_{i=1}^N \\sum_{h_i \\in \\{0,1\\}} \\exp\\left( b_i h_i \\right) \\int_{-\\infty}^\\infty \\exp\\left({1 \\over 2} \\left|\\left| v^T W_i h_i \\alpha_i^{-{1 \\over 2}} \\right|\\right|^2\\right) \\exp\\left( - {1 \\over 2} \\left|\\left| x_i - v^T W_i h_i \\alpha_i^{-{1 \\over 2}} \\right|\\right|^2 \\right) \\mathrm{d}s \\\\ \n",
    "= \\exp\\left( - {1 \\over 2} v^T \\Lambda v \\right) \\prod_{i=1}^N \\sum_{h_i \\in \\{0,1\\}}\\exp\\left({1 \\over 2} \\left|\\left| v^T W_i h_i \\alpha_i^{-{1 \\over 2}} \\right|\\right|^2 + b_i h_i \\right) (2 \\pi)^{{K \\over 2}} det (\\alpha_i^{-{1 \\over 2}}) \\\\  \n",
    "= \\exp\\left( - {1 \\over 2} v^T \\Lambda v \\right) \\prod_{i=1}^N \\left[1 + \\log\\left({1 \\over 2} v^T W_i \\alpha_i^{-1} W_i^T v + b_i \\right) \\right] \\prod_{i=1}^N \\prod_{j=1}^K \\left(2 \\pi \\alpha_{i, jj}^{-1} \\right)^{{1 \\over 2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditionals:\n",
    "$$ p(h_i=1 | v) = \\sigma\\left({1 \\over 2} v^T W_i \\alpha_i^{-1} W_i^Tv + b_i \\right) $$\n",
    "$$ p(s | v, h) = \\prod_{i=1}^N \\mathcal{N} \\left( h_i \\alpha_i^{-1} W_i^Tv, \\alpha_i^{-1} \\right) $$\n",
    "$$ p(v | s, h) = \\mathcal{N} \\left( \\Lambda^{-1} \\sum_{i=1}^N W_i s_i h_i, \\Lambda^{-1} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_h_given_v(self, v):\n",
    "    \"\"\"\n",
    "    v ... visible of shape (batch_size, num_vis {D})\n",
    "    h ... spike hidden of shape (batch_size, num_hid {N}, spike_dim {1}) => (batch_size, num_hid {N})\n",
    "    sampling h ~ p(h_i=1|v) = sigm(0.5 v^TW_{i}\\alpha_i^{-1}W_{i}^Tv + b_i)\n",
    "    \"\"\"\n",
    "    vW = T.tensordot(v, self.W, axes=([1],[0])) # (batch_size, num_hid, num_slab)\n",
    "    h_pre = 0.5 * T.sum(vW ** 2 / self.alpha.dimshuffle('x', 0, 1), axis=-1) + self.b # (batch_size, num_hid)\n",
    "    h_mean = T.nnet.sigmoid(h_pre)\n",
    "    h_samp = self.srng.binomial(size=h_mean.shape, n=1, p=h_mean,\n",
    "                                dtype=theano.config.floatX)\n",
    "    return [h_pre, h_mean, h_samp]\n",
    "\n",
    "def sample_s_given_v_h(self, v, h):\n",
    "    \"\"\"\n",
    "    v ... visible of shape (batch_size, num_vis {D})\n",
    "    h ... spike hidden of shape (batch_size, num_hid {N}, spike_dim {1}) => (batch_size, num_hid {N})\n",
    "    s ... slab hidden of shape (batch_size, num_hid {N}, slab_dim {K})\n",
    "    sampling s ~ p(s|v,h) = \\Prod_{i=1}^N N(h_i \\alpha_i^{-1}W^T_iv, \\alpha_i^{-1})\n",
    "    \"\"\"\n",
    "    vW = T.tensordot(v, self.W, axes=([1],[0])) # (batch_size, num_hid, num_slab)\n",
    "    s_var = self.alpha.dimshuffle('x', 0, 1)\n",
    "    s_mean = s_pre = (h.dimshuffle(0, 1, 'x') / s_var) *  vW\n",
    "    samp = self.srng.normal(size=s_mean.shape, dtype=theano.config.floatX) # mean-field approximation\n",
    "    s_samp = samp / T.sqrt(s_var) + s_mean\n",
    "    return [s_pre, s_mean, s_samp]\n",
    "\n",
    "def sample_v_given_s_h(self, s, h):\n",
    "    \"\"\"\n",
    "    v ... visible of shape (batch_size, num_vis {D})\n",
    "    h ... spike hidden of shape (batch_size, num_hid {N}, spike_dim {1}) => (batch_size, num_hid {N})\n",
    "    s ... slab hidden of shape (batch_size, num_hid {N}, slab_dim {K})\n",
    "    sampling s ~ p(s|v,h) = \\Prod_{i=1}^N N(h_i \\alpha_i^{-1}W^T_iv, \\alpha_i^{-1})\n",
    "    \"\"\"\n",
    "    # hs ~ (batch_size, num_hid, 'x') * (batch_size, num_hid, slab_dim) = (batch_size, num_hid, num_slab)\n",
    "    # v_pre ~ (batch_size, 'x', num_hid, num_slab) * ('x', num_vis, num_hid, num_slab) = (batch_size, num_vis, num_hid, num_slab)\n",
    "    Wsh = T.tensordot(h[:,:,None] * s, self.W, axes=([1,2], [1,2])) # (batch_size, num_vis) T.sum(hs[:,None] * self.W[None], axis=(2,3))\n",
    "    v_var = self.Lambda\n",
    "    v_mean = v_pre = Wsh / v_var # (batch_size, num_vis) / diag(num_vis,) = (batch_size, num_vis)\n",
    "    samp = self.srng.normal(size=v_mean.shape, dtype=theano.config.floatX)\n",
    "    v_samp = samp / T.sqrt(s_var) + v_mean\n",
    "    return [v_pre, v_mean, v_samp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free energy:\n",
    "$$ F(v) = - \\log(p(v)) \\\\  \n",
    "\\propto {1 \\over 2} v^T \\Lambda v - \\sum_{i=1}^N \\log \\left (1 + \\exp\\left({1 \\over 2} v^T W_i \\alpha_i^{-1} W_i^T v + b_i \\right) \\right) - {1 \\over 2} \\sum_{i=1}^N \\sum_{j=1}^K \\log (2 \\pi \\alpha_{i, jj}^{-1}) + \\log Z \\\\\n",
    "= {1 \\over 2} v^T \\Lambda v - \\sum_{i=1}^N softplus \\left ({1 \\over 2} v^T W_i \\alpha_i^{-1} W_i^T v + b_i \\right) - {1 \\over 2} \\sum_{i=1}^N \\sum_{j=1}^K \\log (2 \\pi \\alpha_{i, jj}^{-1}) + \\log Z$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def free_energy_spike_n_slab(self, v):\n",
    "    fe_vis_term = 0.5 * T.dot(v ** 2, self.Lambda) # T.sum((v ** 2) * self.L, axis=-1) # (batch_size, num_vis) x (num_vis,) = (batch_size,)\n",
    "    fe_vW_sqr = T.tensordot(v, self.W, axes=([1],[0])) ** 2 # (batch_size, num_hid, num_slab)\n",
    "    fe_hid_pre = 0.5 * T.sum(vW_sqr / self.alpha[None], axis=-1) + self.b\n",
    "    \n",
    "    # T.log(1 + T.exp(fe_hid_pre)).sum(axis=-1)\n",
    "    fe_hid_term = T.nnet.softplus(fe_hid_pre).sum(axis=1) # (batch_size,...)\n",
    "    \n",
    "    const_term = 0.5 * T.log(2 * np.pi * self.alpha).sum() # sum over num_hid and num_slab (diagonals)\n",
    "    return fe_vis_term - fe_hid_term - const_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\mu$-Spike-&-Slab RBM ($\\mu$-ssRBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = T.fmatrix('v') # (batch_size, num_vis)\n",
    "h = T.fmatrix('h') # (batch_size, num_hid)\n",
    "s = T.fmatrix('s') # (batch_size, num_hid, num_slab)\n",
    "\n",
    "W = shared(np.random.rand(num_vis, num_hid, num_slab), allow_downcast=True)\n",
    "# diagonal matrices in form of vector, use T.linalg./np.diag for full matrix\n",
    "Lambda = shared(np.random.rand(num_vis), allow_downcast=True)\n",
    "alpha = shared(np.random.rand(num_hid, num_slab), allow_downcast=True)\n",
    "\n",
    "Phi = shared(np.random.rand(num_hid, num_vis), allow_downcast=True) # (num_hid, num_vis) -> diag in num_vis\n",
    "mu = shared(np.random.rand(num_hid, num_slab), allow_downcast=True) # (num_hid, num_slab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy function:  \n",
    "$$E(v, s, h) = {1 \\over 2} v^T \\left( \\Lambda + \\sum_{i=1}^N \\Phi_i h_i \\right) v - \\sum^N_{i=1} \\left(v^T W_i s_i h_i - {1 \\over 2} s_i^T \\alpha_i s_i + b_i h_i \\right) - \\sum_{i=1}^N  \\mu_i^T \\alpha_i s_i h_i - \\sum_{i=1}^N \\mu_i^T \\alpha_i \\mu_i h_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visible distribution:  \n",
    "$$p(v, s, h) \\propto \\exp\\left(- E(v, s, h)\\right)$$  \n",
    "Following above derivation:  \n",
    "$$p(v) \\propto \\sum_{h \\in \\{0,1\\}} \\int_{-\\infty}^\\infty \\exp\\left(- E(v, s, h)\\right) \\mathrm{d}s \\\\\n",
    "= \\exp\\left( - {1 \\over 2} v^T \\Lambda v \\right) \\prod_{i=1}^N \\left[1 + \\log\\left(-{1 \\over 2} v^T C_{v|h_i}^{-1} v + v^T W_i \\mu_i + b_i \\right) \\right] \\prod_{i=1}^N \\prod_{j=1}^K \\left(2 \\pi \\alpha_{i, jj}^{-1} \\right)^{{1 \\over 2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditionals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(h_i=1 | v) = \\sigma\\left({1 \\over 2} v^T W_i \\alpha_i^{-1} W_i^Tv + v^TW_i \\mu_i - {1 \\over 2} v^T\\Phi_i v + b_i \\right) $$\n",
    "$$ p(s | v, h) = \\prod_{i=1}^N \\mathcal{N} \\left( (v^TW_i\\alpha_i^{-1} + \\mu_i) h_i, \\alpha_i^{-1} \\right) $$\n",
    "$$ p(v | s, h) = \\mathcal{N} \\left( C_{v|s,h}\\sum_{i=1}^N W_i s_i h_i, C_{v|s,h} ) \\right), \\quad C_{v|s,h} = \\left(\\Lambda + \\sum_{i=1}^N \\Phi_i h_i\\right)^{-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_h_given_v(self, v):\n",
    "    \"\"\"\n",
    "    v ... visible of shape (batch_size, num_vis {D})\n",
    "    h ... spike hidden of shape (batch_size, num_hid {N}, spike_dim {1}) => (batch_size, num_hid {N})\n",
    "    sampling h ~ p(h_i=1|v) = sigm(0.5 v^TW_{i}\\alpha_i^{-1}W_{i}^Tv + b_i)\n",
    "    \"\"\"\n",
    "    vW = T.tensordot(v, self.W, axes=([1],[0])) # (batch_size, num_vis) x (num_vis, num_hid, num_slab) = (batch_size, num_hid, num_slab)\n",
    "    vW_sqr_b = 0.5 * T.sum(vW ** 2 / self.alpha[None], axis=-1) + self.b # (batch_size, num_hid)\n",
    "    vW_mu = T.sum(vW * self.mu.dimshuffle('x', 0, 1), axis=-1) # (batch_size, num_hid, num_slab) x ('x', num_hid, num_slab) = (batch_size, num_hid)\n",
    "    vPhiv = 0.5 * T.dot(v ** 2, self.Phi.T) # (batch_size, num_vis) x (num_vis, num_hid) = (batch_size, num_hid)\n",
    "    h_pre = vW_sqr_b + vW_mu - vPhiv\n",
    "    h_mean = T.nnet.sigmoid(h_pre)\n",
    "    h_samp = self.srng.binomial(size=h_mean.shape, n=1, p=h_mean,\n",
    "                                dtype=theano.config.floatX)\n",
    "    return [h_pre, h_mean, h_samp]\n",
    "\n",
    "def sample_s_given_v_h(self, v, h):\n",
    "    \"\"\"\n",
    "    v ... visible of shape (batch_size, num_vis {D})\n",
    "    h ... spike hidden of shape (batch_size, num_hid {N}, spike_dim {1}) => (batch_size, num_hid {N})\n",
    "    s ... slab hidden of shape (batch_size, num_hid {N}, slab_dim {K})\n",
    "    sampling s ~ p(s|v,h) = \\Prod_{i=1}^N N(h_i \\alpha_i^{-1}W^T_iv, \\alpha_i^{-1})\n",
    "    \"\"\"\n",
    "    vW = T.tensordot(v, self.W, axes=([1],[0])) # (batch_size, num_hid, num_slab)\n",
    "#     s_pre = (h.dimshuffle(0, 1, 'x') / self.alpha.dimshuffle('x', 0, 1)) *  vW\n",
    "    # (batch_size, num_hid, 'x') / ('x', num_hid, num_slab) = (batch_size, num_hid, num_slab)\n",
    "    s_var = self.alpha.dimshuffle('x',0)\n",
    "    s_mean = s_pre = h * (vW / s_var + self.mu.dimshuffle('x',0)) #  (batch_size, num_hid, num_slab) + ('x', num_hid, num_slab)\n",
    "    # mean-field approximation\n",
    "    samp = self.srng.normal(size=s_mean.shape, dtype=theano.config.floatX)\n",
    "    s_samp = samp / T.sqrt(s_var) + s_mean\n",
    "    return [s_pre, s_mean, s_samp]\n",
    "\n",
    "def sample_v_given_s_h(self, s, h):\n",
    "    \"\"\"\n",
    "    v ... visible of shape (batch_size, num_vis {D})\n",
    "    h ... spike hidden of shape (batch_size, num_hid {N}, spike_dim {1}) => (batch_size, num_hid {N})\n",
    "    s ... slab hidden of shape (batch_size, num_hid {N}, slab_dim {K})\n",
    "    sampling s ~ p(s|v,h) = \\Prod_{i=1}^N N(h_i \\alpha_i^{-1}W^T_iv, \\alpha_i^{-1})\n",
    "    \"\"\"\n",
    "    # hs ~ (batch_size, num_hid, 'x') * (batch_size, num_hid, slab_dim) = (batch_size, num_hid, num_slab)\n",
    "    # v_pre ~ (batch_size, 'x', num_hid, num_slab) * ('x', num_vis, num_hid, num_slab) = (batch_size, num_vis, num_hid, num_slab)\n",
    "    C = self.Lambda + T.dot(h, self.Phi) # (num_vis,) + (batch_size, num_hid) x (num_hid, num_vis) = (batch_size, num_vis)\n",
    "    Wsh = T.tensordot(h.dimshuffle(0,1,'x') * s, self.W, axes=([1,2], [1,2])) # (batch_size, num_vis) # T.sum(hs[:,None] * self.W[None], axis=(2,3))\n",
    "    v_mean = v_pre = Wsh / C # (batch_size, num_vis) / [batch_size, diag(num_vis,)] = (batch_size, num_vis)\n",
    "    samp = self.srng.normal(size=v_mean.shape, dtype=theano.config.floatX)\n",
    "    v_samp = samp / T.sqrt(C) + v_mean\n",
    "    return [v_pre, v_mean, v_samp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free energy:  \n",
    "$$ F(v) = - \\log(p(v)) \\\\  \n",
    "= {1 \\over 2} v^T \\Lambda v - \\sum_{i=1}^N \\log \\left (1 + \\exp\\left(-{1 \\over 2} v^T C_{v|h_i}^{-1} v + v^T W_i \\mu_i + b_i \\right) \\right) - {1 \\over 2} \\sum_{i=1}^N \\sum_{j=1}^K \\log (2 \\pi \\alpha_{i, jj}^{-1}) + \\log Z \\\\\n",
    "= {1 \\over 2} v^T \\Lambda v - \\sum_{i=1}^N softplus \\left (-{1 \\over 2} v^T C_{v|h_i}^{-1} v + v^T W_i \\mu_i + b_i \\right) - {1 \\over 2} \\sum_{i=1}^N \\sum_{j=1}^K \\log (2 \\pi \\alpha_{i, jj}^{-1}) + \\log Z \\\\\n",
    "C_{v|h_i}^{-1} = \\left(\\Phi_i - W_i \\alpha_i^{-1}W_i^T\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def free_energy_mu_spike_n_slab(self, v):\n",
    "    fe_vis_term = 0.5 * T.dot(v ** 2, self.Lambda) # (batch_size, num_vis) x (num_vis,) = (batch_size,...)\n",
    "    \n",
    "    fe_vW = T.tensordot(v, self.W, axes=([1],[0])) # (batch_size, num_vis) x (num_vis, num_hid, num_slab) = (batch_size, num_hid, num_slab)\n",
    "    fe_vW_sqr_b = 0.5 * T.sum(vW ** 2 / self.alpha.dimshuffle('x',0), axis=-1) + self.b # (batch_size, num_hid)\n",
    "    fe_vW_mu = T.sum(vW * self.mu.dimshuffle('x', 0, 1), axis=-1) # (batch_size, num_hid, num_slab) x ('x', num_hid, num_slab) = (batch_size, num_hid)\n",
    "    fe_vPhiv = 0.5 * T.dot(v ** 2, self.Phi.T) # (batch_size, num_vis) x (num_vis, num_hid) = (batch_size, num_hid)\n",
    "    fe_hid_pre = = vW_sqr_b + vW_mu - vPhiv # (batch_size, num_hid)\n",
    "    \n",
    "    fe_hid_term = T.nnet.softplus(fe_hid_pre).sum(axis=1) # (batch_size,...) # T.log(1 + T.exp(fe_hid_pre)).sum(axis=-1)\n",
    "    \n",
    "    fe_const_term = 0.5 * T.log(2 * np.pi * self.alpha).sum()\n",
    "    return fe_vis_term - fe_hid_term - const_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-Covariance RBM (mcRBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = shared(np.random.rand(num_vis, num_filt), allow_downcast=True)\n",
    "P = shared(np.random.rand(num_filt, num_hid_c), allow_downcast=True)\n",
    "W = shared(np.random.rand(num_vis, num_hid_m), allow_downcast=True)\n",
    "b_c = shared(np.random.rand(num_hid_c,), allow_downcast=True)\n",
    "b_m = shared(np.random.rand(num_hid_m,), allow_downcast=True)\n",
    "\n",
    "v = T.fmatrix('v') # (batch_size, num_vis)\n",
    "h_m = T.fmatrix('h_m') # (batch_size, num_hid_m)\n",
    "h_c = T.fmatrix('h_c') # (batch_size, num_hid_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* needs a HMC sampler instead of the Gibb's one\n",
    "* needs to solve batchwise system of linear equations to evaluate batchwise matrix inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/adrian/code/theano_wmf/')\n",
    "from theano_wmf import ops\n",
    "\n",
    "from theano.tensor.slinalg import solve as Tsolve\n",
    "from theano.tensor.nlinalg import pinv as Tpinv\n",
    "from scipy.linalg import solve, pinv\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S = T.ftensor3('S')\n",
    "Wh = T.fmatrix('Wh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_solve_expression(A_batch, B_batch):\n",
    "    Ainv_batch = ops.batched_inv(A_batch)\n",
    "    R_batch = ops.batched_dot(Ainv_batch, B_batch) # .dimshuffle(0,1,'x')) # need to turn the A vectors into single-row matrices for this\n",
    "    return R_batch # [:, :, 0] \n",
    "\n",
    "Tsolve = theano.function([S, Wh], batch_solve_expression(S, Wh[:,:,None])[:,:,0], allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "theano_res = Tsolve(np.random.rand(100,500,500), np.random.rand(100,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "theano_res2 = Tsolve(np.random.rand(20,784,784), np.random.rand(20,784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy function:  \n",
    "$$ E^{mc}(v, h^c, h^m) = E^{c}(v, h^c) + E^m(v, h^m)$$  \n",
    "$$ E^{c}(v, h^c) = - {1 \\over 2} \\left({v^T \\over \\| v \\|} C \\text{ diag }\\left(\\{\\| C_{:,k} \\|\\}_{k=1}^N\\right)^{-1}\\right)^2 P h^c  - b_c^T h^c + {1 \\over 2} v^Tv$$\n",
    "$$ E^m(v, h^m) = - v^TWh^m - v^Tb_m$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visible distribution:  \n",
    "$$p(v, h^c, h^m) \\propto \\exp\\left(- E(v, h^c, h^m)\\right)$$  \n",
    "$$ p(v) \\propto \\sum\\limits_{h^c, h^m} \\exp\\left(- E(v, h^c, h^m) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditionals:  \n",
    "$$ p(h^m | v) = \\sigma\\left( v^T W + b_v \\right ) $$\n",
    "$$ p(h^c | v) = \\sigma\\left( {1 \\over 2} \\left({v^T \\over \\| v \\|} C \\text{ diag }\\left(\\{\\| C_{:,k} \\|\\}_{k=1}^N\\right)^{-1}\\right)^2 P + b_c - {1 \\over 2} v^T v\\right)$$\n",
    "$$ p(v | h^m, h^c) = N\\left(\\Sigma (W h^c), \\Sigma \\right), \\quad \\Sigma = C \\text{ diag } (P h^c) C^T)^{-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_hm_given_v(self, v):\n",
    "    \"\"\"\n",
    "    v ... visible of shape (batch_size, num_vis {D})\n",
    "    h ... hidden of shape (batch_size, num_hid {N}, spike_dim {1}) => (batch_size, num_hid {N})\n",
    "    sampling h ~ p(h_i=1|v) = sigm(0.5 v^TW_{i}\\alpha_i^{-1}W_{i}^Tv + b_i)\n",
    "    \"\"\"\n",
    "    h_pre = v.dot(self.W) + self.b_m # (batch_size, num_hid_m)\n",
    "    h_mean = T.nnet.sigmoid(h_pre)\n",
    "    h_samp = self.srng.binomial(size=h_mean.shape, n=1, p=h_mean,\n",
    "                                dtype=theano.config.floatX)\n",
    "    return [h_pre, h_mean, h_samp]\n",
    "\n",
    "def sample_hc_given_v(self, v):\n",
    "    \"\"\"\n",
    "    v ... visible of shape (batch_size, num_vis {D})\n",
    "    h ... spike hidden of shape (batch_size, num_hid {N}, spike_dim {1}) => (batch_size, num_hid {N})\n",
    "    sampling h ~ p(h_i=1|v) = sigm(0.5 v^TW_{i}\\alpha_i^{-1}W_{i}^Tv + b_i)\n",
    "    \"\"\"\n",
    "    h_pre = 0.5 * (v.dot(self.C)**2).dot(self.P) + self.b_c # (batch_size, num_hid_c)\n",
    "    h_mean = T.nnet.sigmoid(h_pre)\n",
    "    h_samp = self.srng.binomial(size=h_mean.shape, n=1, p=h_mean,\n",
    "                                dtype=theano.config.floatX)\n",
    "    return [h_pre, h_mean, h_samp]\n",
    "\n",
    "def sample_v_given_hm_hc(self, h_m, h_c):\n",
    "    \"\"\"\n",
    "    v ... visible of shape (batch_size, num_vis {D})\n",
    "    h ... spike hidden of shape (batch_size, num_hid {N}, spike_dim {1}) => (batch_size, num_hid {N})\n",
    "    s ... slab hidden of shape (batch_size, num_hid {N}, slab_dim {K})\n",
    "    sampling s ~ p(s|v,h) = \\Prod_{i=1}^N N(h_i \\alpha_i^{-1}W^T_iv, \\alpha_i^{-1})\n",
    "    \"\"\"\n",
    "    hP = h_c.dot(self.P.T)[:,None] # (batch_size, num_hid_c) x (num_hid_c, num_filt) = (batch_size, 'x', num_filt)\n",
    "    # [('x', num_vis, num_filt) * (batch_size, 'x', num_filt)] x (num_filt, num_vis) \n",
    "    Sigma = T.tensordot(self.C[None] * hP, self.C.T, axes=([-1], [0])) # (batch_size, num_vis, num_vis)\n",
    "    Wh_m = T.dot(h_m, self.W.T) # (batch_size, num_vis)\n",
    "    v_mean = v_pre = Tsolve(Sigma, Wh_m[:,:,None])[:,:,0] # (batch_size, num_vis, ...)\n",
    "    samp = self.srng.normal(size=v_mean.shape, dtype=theano.config.floatX)\n",
    "    v_samp = Tsolve(T.sqrt(Sigma), samp[:,:,None])[:,:,0] + v_mean \n",
    "    return [v_pre, v_mean, v_samp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free energy:  \n",
    "$$ F(v) = - \\log p(v) = − \\sum\\limits_{k} \\log\\left(1 + \\exp\\left( {1 \\over 2} (v^T C_{:,k})^2 P \\right) + b^c_k \\right) −\n",
    "\\sum\\limits_{j} \\log\\left(1 + \\exp\\left(v^T W_{:,j} + b^m_j \\right) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors description of the HMC sampling procedure:  \n",
    "\"HMC draws a sample by simulating a particle moving on\n",
    "this free energy surface. The particle starts at the data-point\n",
    "and is given an initial random momentum sampled from a\n",
    "spherical Gaussian with unit variance. It then moves over\n",
    "the surface using the gradient of the free energy to determine\n",
    "its acceleration. If the simulation is accurate, the sum\n",
    "of the potential and kinetic energies will not change and the\n",
    "results of the simulation can be accepted. If the total energy\n",
    "rises during the simulation, the result is accepted with\n",
    "a probability equal to the negative exponential of the total\n",
    "energy increase. Numerical errors can be reduced by using\n",
    "“leapfrog” steps. After simulating the trajectory\n",
    "for a number of leapfrog steps, the current momentum\n",
    "is discarded and a new momentum is sampled from a spherical\n",
    "Gaussian. This Markov chain will eventually sample\n",
    "from the correct distribution, but we only need to run it for\n",
    "a small fraction of the time this would take.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def free_energy_mean(self, v):\n",
    "    fe_m_pre = v.dot(self.W) + self.b_m\n",
    "    return - T.sum(T.softplus(fe_m_pre), axis=-1) # (batch_size,)\n",
    "\n",
    "def free_energy_cov(self, v):\n",
    "    v_norm = T.sum(v**2, axis=-1)\n",
    "    C_norm = T.sum(self.C**2)\n",
    "    C_normed = C / C_norm\n",
    "    v_normed = v / v_norm\n",
    "    fe_c_pre = 0.5 * (v_normed.dot(C_normed)**2).dot(self.P) + self.b_c + 0.5 * v_norm\n",
    "    return - T.sum(T.softplus(fe_c_pre), axis=-1) # (batch_size,)\n",
    "\n",
    "def free_energy(self, v):\n",
    "    fe_m_term = self.free_energy_mean(v)\n",
    "    fe_c_term = self.free_energy_cov(v)\n",
    "    return fe_c_term + fe_m_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm:  \n",
    "1) compute the gradient of the free energy at the training samples  \n",
    "2) get samples by using CD with HMC  \n",
    "3) compute the gradient of the free energy at the HMC samples  \n",
    "4) update the parameters by taking the difference of the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:  \n",
    "http://www.cs.toronto.edu/~fritz/absps/ranzato_cvpr2010.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Product of Student t's (mPoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = shared(np.random.rand(num_vis, num_hid_c))\n",
    "W = shared(np.random.rand(num_vis, num_hid_m))\n",
    "gamma = shared(np.zeros(num_hid_c,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy function:  \n",
    "$$ E^{mPoT}(v, h^c, h^m) = E^{PoT}(v, h^c) + E^m(v, h^m)$$  \n",
    "$$ E^{PoT}(v, h^c) = \\left(1 + {1 \\over 2}(v^T C)^2\\right)P h^c + (1-\\gamma)^T\\log h^c$$\n",
    "$$ E^m(v, h^m) = {1 \\over 2} v^Tv - v^TWh^m - b^T_m h^m - v^T b_v$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visible distribution (recipe for extending any distribution by including hidden variables):  \n",
    "$$ p(v, h^c, h^m) \\propto \\exp\\left(-E^{mPoT}(v, h^c, h^m) \\right) $$  \n",
    "$$ p(v) \\propto \\left( 1 + {1 \\over 2} (v^T C)^2 \\right)^{-\\gamma} = \\exp \\left( -\\log(1 + {1 \\over 2} (v^T C)^2)^T \\gamma \\right) \\\\ \n",
    "= \\sum_{h^m \\in \\{0,1\\}^N} \\int_{-\\infty}^\\infty \\exp\\left(-E^{mPoT}(v, h^c, h^m) \\right) \\mathrm{d}h^c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditionals:  \n",
    "$$ p(h^m | v) = \\sigma(v^T W + b_m)$$\n",
    "$$ p(h^c | v) = \\Gamma \\left(\\alpha=\\gamma, \\beta=1 + {1 \\over 2} (v^T C)^2 P \\right) \n",
    "=> E[h^c | v] = {\\alpha \\over \\beta} = {\\gamma \\over 1 + {1 \\over 2} (v^T C)^2 P}$$\n",
    "$$ p(v | h^m, h^c) = N \\left( \\Sigma(W h^m + b_v), \\Sigma \\right), \\quad \\Sigma^{-1} = C \\text{ diag } (P h^c) C^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free energy:\n",
    "$$ F(v) = - \\log p(v) = \\sum\\limits^N_{j=1} \\log\\left(1 + {1 \\over 2} \\left(v^T C \\right)^2 P_{:,j} \\right) + {1 \\over 2} v^T v - \\sum\\limits^M_{k=1} \\log\\left(1 + \\exp\\left(v^T W_{:,k} + b^m_{k} \\right) \\right) - v^Tb_v$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HMC sampling:  \n",
    "def draw_sample(datainit, param, layer): # only 1 M.C. step\n",
    "    if layer == 1: # 1st layer: do 1 step of HMC\n",
    "        velocity = np.random.randn()\n",
    "        tot_energy1 = .5*velocity**2 + compute_F(datainit,param)\n",
    "        data = datainit\n",
    "        velocity = velocity - eps * compute_dFdX(data,param)/2\n",
    "        for iter in range(20): # 20 leap-frog steps\n",
    "            data = data + eps * velocity\n",
    "            if iter != 19:\n",
    "                velocity = velocity - eps * compute_dFdX(data,param)\n",
    "        velocity = velocity - eps * compute_dFdX(data,param)\n",
    "        tot_energy2 = .5*velocity**2 + compute_F(data,param)\n",
    "        if rand() < exp(tot_energy1 - tot_energy2):\n",
    "            return data # accept sample\n",
    "        else:\n",
    "            return datainit # reject sample\n",
    "    else: # higher layers: do 1 step of Gibbs\n",
    "        hiddens = sample_posterior(datainit,param) # p(h|x)\n",
    "        return sample_inputs(hiddens,param) # p(x|h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:  \n",
    "http://www.cs.toronto.edu/~fritz/absps/PoT.pdf (PoT)  \n",
    "http://www.cs.toronto.edu/~fritz/absps/mcimage.pdf  \n",
    "https://www.cs.toronto.edu/~vmnih/docs/ranzato_pami13.pdf (extended version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/benanne/morb/blob/master/morb/samplers.py  \n",
    "https://groups.google.com/forum/#!topic/theano-users/ytqep8AickA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## approximate gamma sampler\n",
    "# Two approximations for the gamma function are defined.\n",
    "# Windschitl is very fast, but problematic close to 0, and using the reflection formula\n",
    "# causes discontinuities.\n",
    "# Lanczos on the other hand is extremely accurate, but slower.\n",
    "   \n",
    "def _log_gamma_windschitl(z):\n",
    "    \"\"\"\n",
    "    computes log(gamma(z)) using windschitl's approximation.\n",
    "    \"\"\"\n",
    "    return 0.5 * (T.log(2*np.pi) - T.log(z)  + z * (2 * T.log(z) - 2 + T.log(z * T.sinh(1/z) + 1 / (810*(z**6)))))\n",
    "    \n",
    "def _log_gamma_ratio_windschitl(z, k):\n",
    "    \"\"\"\n",
    "    computes log(gamma(z+k)/gamma(z)) using windschitl's approximation.\n",
    "    \"\"\"\n",
    "    return _log_gamma_windschitl(z + k) - _log_gamma_windschitl(z)\n",
    "    \n",
    "\n",
    "def _log_gamma_lanczos(z):\n",
    "    # optimised by nouiz. thanks!\n",
    "    assert z.dtype.startswith(\"float\")\n",
    "    # reflection formula. Normally only used for negative arguments,\n",
    "    # but here it's also used for 0 < z < 0.5 to improve accuracy in\n",
    "    # this region.\n",
    "    flip_z = 1 - z\n",
    "    # because both paths are always executed (reflected and\n",
    "    # non-reflected), the reflection formula causes trouble when the\n",
    "    # input argument is larger than one.\n",
    "    # Note that for any z > 1, flip_z < 0.\n",
    "    # To prevent these problems, we simply set all flip_z < 0 to a\n",
    "    # 'dummy' value. This is not a problem, since these computations\n",
    "    # are useless anyway and are discarded by the T.switch at the end\n",
    "    # of the function.\n",
    "    flip_z = T.switch(flip_z < 0, 1, flip_z)\n",
    "    log_pi = np.asarray(np.log(np.pi), dtype=z.dtype)\n",
    "    small = log_pi - T.log(T.sin(np.pi * z)) - _log_gamma_lanczos_sub(flip_z)\n",
    "    big = _log_gamma_lanczos_sub(z)\n",
    "    return T.switch(z < 0.5, small, big)\n",
    "\n",
    "\n",
    "def _log_gamma_lanczos_sub(z): # expanded version\n",
    "    # optimised by nouiz. thanks!\n",
    "    # Coefficients used by the GNU Scientific Library\n",
    "    # note that vectorising this function and using .sum() turns out to be\n",
    "    # really slow! possibly because the dimension across which is summed is\n",
    "    # really small.\n",
    "    g = 7\n",
    "    p = np.array([0.99999999999980993, 676.5203681218851, -1259.1392167224028,\n",
    "                  771.32342877765313, -176.61502916214059, 12.507343278686905,\n",
    "                  -0.13857109526572012, 9.9843695780195716e-6,\n",
    "                  1.5056327351493116e-7], dtype=z.dtype)\n",
    "    z = z - 1\n",
    "    x = p[0]\n",
    "    for i in range(1, g + 2):\n",
    "        x += p[i] / (z + i)\n",
    "    t = z + g + 0.5\n",
    "    pi = np.asarray(np.pi, dtype=z.dtype)\n",
    "    log_sqrt_2pi = np.asarray(np.log(np.sqrt(2 * np.pi)), dtype=z.dtype)\n",
    "    return log_sqrt_2pi + (z + 0.5) * T.log(t) - t + T.log(x)\n",
    "\n",
    "    \n",
    "def _log_gamma_ratio_lanczos(z, k):\n",
    "    \"\"\"\n",
    "    computes log(gamma(z+k)/gamma(z)) using the lanczos approximation.\n",
    "    \"\"\" \n",
    "    return _log_gamma_lanczos(z + k) - _log_gamma_lanczos(z)\n",
    "    \n",
    " \n",
    "def gamma_approx(k, theta=1):\n",
    "    \"\"\"\n",
    "    Sample from a gamma distribution using the Wilson-Hilferty approximation.\n",
    "    The gamma function itself is also approximated, so everything can be\n",
    "    computed on the GPU (using the Lanczos approximation).\n",
    "    \"\"\"\n",
    "    lmbda = 1/3.0 # according to Wilson and Hilferty\n",
    "    mu = T.exp(_log_gamma_ratio_lanczos(k, lmbda))\n",
    "    sigma = T.sqrt(T.exp(_log_gamma_ratio_lanczos(k, 2*lmbda)) - mu**2)\n",
    "    normal_samples = theano_rng.normal(size=k.shape, avg=mu, std=sigma, dtype=theano.config.floatX)\n",
    "    gamma_samples = theta * T.abs_(normal_samples ** 3)\n",
    "    # The T.abs_ is technically incorrect. The problem is that, without it, this formula may yield\n",
    "    # negative samples, which is impossible for the gamma distribution.\n",
    "    # It was also noted that, for very small values of the shape parameter k, the distribution\n",
    "    # of resulting samples is roughly symmetric around 0. By 'folding' the negative part\n",
    "    # onto the positive part, we still get a decent approximation because of this.\n",
    "    return gamma_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_hm_given_v(self, v):\n",
    "    \"\"\"\n",
    "    v ... visible of shape (batch_size, num_vis {D})\n",
    "    h ... hidden of shape (batch_size, num_hid {N}, spike_dim {1}) => (batch_size, num_hid {N})\n",
    "    sampling h ~ p(h_i=1|v) = sigm(0.5 v^TW_{i}\\alpha_i^{-1}W_{i}^Tv + b_i)\n",
    "    \"\"\"\n",
    "    h_pre = v.dot(self.W) + self.b_m # (batch_size, num_hid_m)\n",
    "    h_mean = T.nnet.sigmoid(h_pre)\n",
    "    h_samp = self.srng.binomial(size=h_mean.shape, n=1, p=h_mean,\n",
    "                                dtype=theano.config.floatX)\n",
    "    return [h_pre, h_mean, h_samp]\n",
    "\n",
    "def sample_hc_given_v(self, v):\n",
    "    \"\"\"\n",
    "    v ... visible of shape (batch_size, num_vis {D})\n",
    "    h ... spike hidden of shape (batch_size, num_hid {N}, spike_dim {1}) => (batch_size, num_hid {N})\n",
    "    sampling h ~ p(h_i=1|v) = sigm(0.5 v^TW_{i}\\alpha_i^{-1}W_{i}^Tv + b_i)\n",
    "    \"\"\"\n",
    "    # Gamma sampler\n",
    "    alpha = self.gamma # (num_hid_c,)\n",
    "    beta = 1 + 0.5 * (v.dot(C)**2) # (batch_size, num_hid_c)\n",
    "    k = alpha\n",
    "    theta = 1 / beta\n",
    "    h_mean = k * theta # alpha / beta\n",
    "    h_samp = gamma_approx(k, theta) # k == alpha, theta == 1/beta\n",
    "    return [None, h_mean, h_samp]\n",
    "\n",
    "def sample_v_given_hm_hc(self, h_m, h_c):\n",
    "    \"\"\"\n",
    "    v ... visible of shape (batch_size, num_vis {D})\n",
    "    h_m ... mean hidden of shape (batch_size, num_hid {N})\n",
    "    h_c ... cov hidden of shape (batch_size, num_hid {N})\n",
    "    sampling v ~ p(v|h_m,h_c) = \n",
    "    \"\"\"\n",
    "    ChC = T.tensordot(self.C[None] * h_c[:,None], self.C.T, axes=([-1], [0])) \n",
    "    # ('x', num_vis, num_hid_c) * (batch_size, 'x', diag(num_hid_c)) x (num_hid_c, num_vis) = (batch_size, num_vis, num_vis)\n",
    "    Sigma_c = theano.scan(lambda x: T_inv(x), ChC) \n",
    "    Sigma = Sigma_c + T.eye(m=C.shape[0])[None] # (batch_size, num_vis, num_vis)\n",
    "    Wh_m = T.dot(h_m, self.W.T) # (batch_size, num_hid_m) x (num_hid_m, num_vis) = (batch_size, num_vis)\n",
    "    v_mean = v_pre = Tsolve(Sigma, Wh_m[:,:,None])[:,:,0] # (batch_size, num_vis, ...)\n",
    "    # (batch_size, num_vis, num_vis)^-1 x (batch_size, num_vis, 'x') = (batch_size, num_vis)\n",
    "    samp = self.srng.normal(size=v_mean.shape, dtype=theano.config.floatX)\n",
    "    v_samp = Tsolve(T.sqrt(Sigma), samp[:,:,None])[:,:,0] + v_mean \n",
    "    return [v_pre, v_mean, v_samp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def free_energy_pot_mean(self, v):\n",
    "    fe_m_pre = v.dot(self.W) + self.b_m\n",
    "    return - T.sum(T.softplus(fe_m_pre), axis=-1) # (batch_size,)\n",
    "\n",
    "def free_energy_pot_cov(self, v):\n",
    "    v_norm = T.sum(v**2, axis=-1) # (batch_size,)\n",
    "    fe_c_pre = 1 + 0.5 * (v.dot(C)**2) # (batch_size, num_hid_c)\n",
    "    return T.sum(T.log(fe_c_pre) * self.gamma, axis=-1) + 0.5 * v_norm # (batch_size,)\n",
    "\n",
    "def free_energy(self, v):\n",
    "    fe_m_term = self.free_energy_mean(v)\n",
    "    fe_c_term = self.free_energy_cov(v)\n",
    "    return fe_c_term + fe_m_term"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
