{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf  \n",
    "http://homepages.inf.ed.ac.uk/imurray2/pub/08dbn_ais/dbn_ais.pdf  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting the learning signal for $CD_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. the hidden units driven by data = always use stochastic binary states\n",
    "2. the hidden units driven by reconstructions, always use probabilities without sampling.\n",
    "3. the visible units using the logistic function = real-valued probabilities for both data and reconstructions\n",
    "4. for the pairwise/individual statistics for learning weights/biases = the probabilities, not the binary states and make sure the weights have random initial values to break symmetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting the size of the mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use mini-batches of size about 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to use reconstruction error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use it but don’t trust it\n",
    "- to really know what is going on = use multiple histograms and graphic displays as described in section 15\n",
    "- use Annealed Importance Sampling to estimate the density on held out data\n",
    "- when learning a joint density model of labelled data monitor the discriminative performance on the training data and on a held out validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### monitoring the overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for overfitting model - the average validation free energy would rise relatively to the average training free energy and this gap represents the amount of overfitting\n",
    "- always use the same training subset\n",
    "- if the gap grows, the model is overfitting, though the probability of the validation set may still improve\n",
    "- use the same weights when computing the two averages that you wish to compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting the learning rates for weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updates should be about $10^{−3}$ times the weights - weight:update ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting the initial values of the weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use small random values for the weights chosen from a zero-mean Gaussian with a standard deviation of 0.01\n",
    "- set the hidden biases to 0, the visible biases to $\\log[p_i/(1−p_i)]$ where $p_i$'s the proportion of training vectors in which unit i is on\n",
    "- look at the activities of the hidden units occasionally to check that they are not always on or of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- start with a momentum of 0.5\n",
    "- once the reconstruction error settled down to gentle progress, increase to 0.9\n",
    "- at first, expect increased reconstruction error\n",
    "- if it pertains, keep reducing the learning rate by factors of 2 until the instability disappears"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using L2 weight-decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sensible values typically range from 0.01 to 0.00001\n",
    "- try an initial weight-cost of 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- set the sparsity target $p$ to between 0.01 and 0.1\n",
    "- sparsity penalty $\\approx −p \\log q − (1 − p) \\log(1 − q)$ with exp decay mean $q_{new} = \\lambda q_{old} + (1 − \\lambda)q_{current}$\n",
    "- set the decay-rate, $\\lambda$, of the estimated value of q between 0.9 and 0.99\n",
    "- histogram the mean activities of the hidden units $\\&$ set the sparsity-cost s.t. the hidden units have mean probabilities close to the target\n",
    "- if probabilities tightly clustered around the target value, reduce the sparsity-cost to interfere it less with the main objective of the learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choosing the number of hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the main issue is overfitting rather than the amount of computation at training or\n",
    "test time, estimate how many bits it would take to describe each data-vector if you were using a good\n",
    "model (i.e. estimate the typical negative log2 probability of a datavector under a good model). Then\n",
    "multiply that estimate by the number of training cases and use a number of parameters that is about\n",
    "an order of magnitude smaller. If you are using a sparsity target that is very small, you may be able\n",
    "to use more hidden units. If the training cases are highly redundant, as they typically will be for very\n",
    "big training sets, you need to use fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
