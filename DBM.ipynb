{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Boltzman Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data-dependent expectation $\\mathbb{E}_{P_{data}}$ - $P_{data}(h, v; \\theta) = p(h|v; \\theta) P_{data}(v)$, with \n",
    "$P_{data}(v) = {1 \\over N} \\sum\\limits_n \\delta(v - v_n) $  \n",
    "Model-dependent expectation $\\mathbb{E}_{P_{model}}$ - $P_{model} = p(h|v; \\theta) p(v; \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the model-dependent expectations using Persistent Markov Chains  \n",
    "* Stochatistc Approximation Procedure (Stochastic Maximum Likelihood)  \n",
    "    1. Given $X_t$, sample a new state $X_{t+1}$ from a transition operator $T^θ_t(X_{t+1}; X_t)$ that leaves $p^θ_t$ invariant.  \n",
    "    2. Obtain a new parameter $θ_{t+1}$ by using the expectation with respect to $X_{t+1}$ instead of the intractable model’s expectation.  \n",
    "* sufficient condition for a.s. convergence - the learning rate has to decrease with time:\n",
    "    * $\\sum\\limits^∞_{t=0} α_t = ∞$ and $\\sum\\limits^∞_{t=0} α^2_t < ∞$.\n",
    "    * e.g. $α_t = 1/t$\n",
    "* many parallel persistent chains\n",
    "* a “fantasy” particle is the current state in each of these chains "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the data-dependent expectations using variational lower bound  \n",
    "* posterior $ p(h|v) $ approximated by $ q(h|v; \\mu) $  \n",
    "* the variationial lower bound of the log-likelihood:  \n",
    "    $$ \\log p(v; \\theta) = \\log \\sum_h p(v|h; \\theta) p(h)  \\geq \\sum_h q(h|v; \\mu) \\log p(v, h; \\theta) + \\mathcal{H}(q) \\\\\n",
    "    = \\mathbb{E}_{q(h|v)} \\log  p(v,h; \\theta) - KL\\left[q(h|v) || p(h|v) \\right] \n",
    "    = \\log p(v; \\theta) - KL[q(h|v; \\mu)||p(h|v; \\theta)]$$  \n",
    "* a simple mean-field approximation $ q(h; \\mu) = \\prod^P_{j=1} q(h_i) = \\prod^P_{j=1} \\mu_i^{h_i}$, where $q(h_i = 1) = \\mu_i$ and\n",
    "$P$ is a number of hidden units\n",
    "* optimizing the bound after plugging-in above mf:\n",
    "    $$ \\log p(v; \\theta) \\geq {1 \\over 2} \\sum_{i,k} L_{ik}v_iv_k +\n",
    "        {1 \\over 2} \\sum_{j,m} J_{j,m} \\mu_j \\mu_m +\n",
    "        \\sum_{i,j} W_{i,j} v_i \\mu_j - \\log Z(\\theta) +\n",
    "        \\sum_j [\\mu_j \\log \\mu_j + (1 - \\mu_j ) \\log (1 - \\mu_j )] $$\n",
    "where $L=J=0$ for a Restricted BM:\n",
    "    $$  \\log p(v; \\theta) \\geq         \n",
    "        \\sum_{i,j} W_{i,j} v_i \\mu_j + \\sum_j [\\mu_j \\log \\mu_j + (1 - \\mu_j ) \\log (1 - \\mu_j )] - \\log Z(\\theta) $$\n",
    "yields the mean-field fixed-point equations for updating of the variational parameter  \n",
    " $$ \\mu_j \\leftarrow \\sigma \\left( \\sum_i W_{i,j}v_i + \\sum_{m \\backslash j} J_{m,j} \\mu_m \\right)$$\n",
    "    * the 2nd term in the inequality represents the cost to maximize, where \n",
    "    $$Z(\\theta) = \\sum\\limits_{v, h} p(v | h; \\theta) * q(h)$$ \n",
    "    corresponds to the expected model-dependent energy and the rest of substracted terms corresponds to the expected data-dependent energy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm:  \n",
    "1. randomly initialize parameters $\\theta_0$ and fantasy particles $\\left\\{\\left(\\tilde{v}_0^i, \\tilde{h}_0^i\\right) \\right\\}_{i=1}^M$  \n",
    "2. for number of iterations:  \n",
    "    1. model-dependent inference part, i.e. for each $v^n$:\n",
    "        * randomly initialize $\\mu$ and run mean-field updates until convergence to obtain $\\mu^n=\\mu$\n",
    "    2. data-dependent inference part - for each fantasy particle:  \n",
    "        * run a Gibbs sampler to obtain a new state $\\left(\\tilde{v}_{t+1}^i, \\tilde{h}_{t+1}^i\\right)$ given the previous one $\\left(\\tilde{v}_t^i, \\tilde{h}_{t}^i\\right)$\n",
    "3. update the parameters $\\theta_{t+1}$ using the gradients of the difference between the model-dependent and data-dependent (free) energies  \n",
    "4. decrease the learning rate $\\alpha_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter initialization:  \n",
    "    * Weight doubling to eliminate the double-counting problem when top-down and bottom-up influences are subsequently combined - double the input and tie the visibleto-hidden weights for lower-level layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:  \n",
    "http://www.cs.toronto.edu/~fritz/absps/dbm.pdf  \n",
    "http://www.cs.toronto.edu/~rsalakhu/papers/dbmrec.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: RBM as a single layer DBM  \n",
    "$$ p(h_i = 1|v) = \\sigma(v^T W_{:,i} + b_i) $$  \n",
    "$$ p(v_j = 1|h) = \\sigma(W_{j,:} h + b_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-prediction DBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a vector containing all observed variables:\n",
    "    * unsupervised learning $O = v$\n",
    "    * supervised learning $O = [v, y]^T$, where y is available only during training\n",
    "    \n",
    "* a training set $D = \\{ O_i \\}_{i=1}^N$\n",
    "* a sequence of subsets of the possible indices of O, i.e. $S = (S_j)_{j}$, where $S_j = \\{k \\; | \\; O_{j, k} \\} $\n",
    "* $Q_i$ a variational (mean-field) approximation to the joint of $P(O_{S_i}, h | O_{-S_i})$, such that\n",
    "$$ Q_i(O_{S_i} , h) = \\mathop{\\arg \\min}_{Q} KL(Q(O_{S_i}, h) \\| P(O_{S_i}, h | O_{−S_i}))$$\n",
    "* constraining Q to be factorial, i.e. $$ Q(O_{S_i}, h; \\mu) = \\prod^P_{j=1} q(h_j; \\mu_j) = \\prod^P_{j=1} \\mu_j^{h_j}$$ where\n",
    "$$ \\mu_j \\leftarrow \\sigma \\left( \\sum_i W_{i,j}v_i + \\sum_{m \\backslash j} J_{m,j} \\mu_m \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:\n",
    "$$ J(D, \\theta) = - \\sum_{O \\in D} \\sum_i \\log Q_i(O_{S_i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "0. important to preserve the order of sampling \n",
    "1. sample different subsets S_i n-times (large number)\n",
    "2. run n mean-field fixed point equations as recurrent nets for fixed number of steps\n",
    "3. construct n predictions of distribution Q\n",
    "4. take geometric mean and renormalize to estimate final distribution - for efficient approximation average at each step of\n",
    "inference, instead of the entire inference process for correct geometric mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The multi-inference trick\n",
    "* each RNN we average over solves a different inference problem\n",
    "* half of the cases, vi is observed, i.e. contributes $v_iW_{ij}$ to $h_j$’s input \n",
    "* other half of the cases, vi is inferred\n",
    "* for $r_i$ representing the mean field estimate of $v_i$, then contributes $r_iW_{ij}$ to $h_j$’s input, replacing references to v with $0.5(v + r)$, where r is updated at each mean field iteration\n",
    "* a good way to incorporate information from many RNNs trained in slightly different ways\n",
    "* can also be understood as including an input denoising step built into the inference\n",
    "* in practice, multi-inference used for the network trained without running mean field up to convergence, while for a net trained with converged mean field, each RNN is just solving an optimization problem in a graphical model\n",
    "* the multi-inference trick is mostly useful as a cheap alternative for fast training and evaluation rather than high test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering  \n",
    "* center each variable (hidden or visible) by an offset\n",
    "* helps to train without pre-training\n",
    "* alone not good for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize parameters and fantasy particles\n",
    "def init_params():\n",
    "    # init W_i, b_i for layer i\n",
    "    pass\n",
    "\n",
    "def init_fantasy():\n",
    "    # init vis_0\n",
    "    # init hid_0\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model-dependent part\n",
    "def init_mean_field(below=None, W=None, b=None, above=None, W_above=None):\n",
    "    # initialize the current layer from the layer below by using double weights\n",
    "    return T.dot(below2*W)\n",
    "\n",
    "# mean-field fixed equations step\n",
    "def mean_field_iter(below, W, b, above=None, W_above=None):\n",
    "    # markov blanket for layer i\n",
    "    if above is not None and W_above is not None:\n",
    "        mu = T.dot(below.T, W) + T.dot(above, mu_above) + b\n",
    "    else:\n",
    "        \n",
    "    \n",
    "# iteration\n",
    " \n",
    "# model-dependent energy\n",
    "def model_energy(vis):\n",
    "    # using samples of vis and variational pars mu\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data-dependent energy\n",
    "def data_energy(vis):\n",
    "    # using samples of vis and hid\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(0, len(dbm.hidden_layers) - 1):\n",
    "    # do double weights update for_layer_i\n",
    "    if i == 0:\n",
    "        H_hat.append(dbm.hidden_layers[i].mf_update(\n",
    "            state_above=None,\n",
    "            double_weights=True,\n",
    "            state_below=dbm.visible_layer.upward_state(V),\n",
    "            iter_name='0'))\n",
    "    else:\n",
    "        H_hat.append(dbm.hidden_layers[i].mf_update(\n",
    "            state_above=None,\n",
    "            double_weights=True,\n",
    "            state_below=dbm.hidden_layers[\n",
    "                i - 1].upward_state(H_hat[i - 1]),\n",
    "            iter_name='0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reference:  \n",
    "http://www.cs.toronto.edu/~fritz/absps/dbm.pdf  \n",
    "http://www.cs.toronto.edu/~rsalakhu/papers/dbmrec.pdf  \n",
    "http://www-etud.iro.umontreal.ca/~goodfeli/mp_dbm.pdf  \n",
    "http://arxiv.org/pdf/1301.3568.pdf  \n",
    "https://github.com/lisa-lab/pylearn2/tree/master/pylearn2/models/dbm  \n",
    "https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/models/dbm/dbm.py  \n",
    "https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/tests/test_dbm_metrics.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
